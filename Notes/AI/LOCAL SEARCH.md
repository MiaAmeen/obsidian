[[lecture06.pdf]]

- Doesnt care about the path! Only cares about final state.
- Doesnt keep track of paths/states that've been reached.
- From a state, simply explores neighbor states
- Solves optimization problems: aims to maximize/minimize an objective function
-  (1) they use very little memory and (2) they can often find reasonable solutions in large or infinite state spaces for which systematic algorithms are unsuitable.
##### Hill Climbing
![[Screenshot 2025-01-28 at 8.48.23 AM.png]]

4-Queen Problem
![[Screenshot 2025-02-24 at 11.45.36 AM.png]]

###### Issues with Hill Climbing
Local maxima (stuck at suboptimal solution), plateaus (flatline - which way do I go next? same values to the left and right), and ridges (fuck if i know)
###### --> Stochastic Hill Climbing
- random selection among the uphill moves
- selection probability proportional to the steepness
- success depends on chance. (stochastic is opposite of deterministic)
###### --> Random walk hill climbing
- Greedy: with probability p, move to a neighbor with largest value
- Random: With probability 1 - p, move to a random neighbor
###### --> Random-restart
- multiple times with a randomly generated initial state (which is the best maxima)
###### Simulated Annealing
- A "temperature" variable is set to a high initial value and decreased in each iteration
- similar to hill climb but picks random move instead of best
- if move improves objective function, it is accepted
- else accept the move with a probability that decreases with the badness of the move and with the temperature
- If temperature is decreased slowly enough we may get good solution ?
![[Screenshot 2025-01-28 at 9.25.29 AM.png]]
###### Local Beam Search
- Instead of keeping 1 state, keep k best states
- Start with k randomly selected states
- Generate successors of the k states
- If any of the successors is goal, return that one
- Else select k best states from the successors and repeat
- What if all k states end up on the same local maxima?
	- stochastic beam search is randomly biased toward the good k successors
- Similar to natural selection!

Genetic Algorithms
- variant of stochastic beam search, successors generated by "combining two parents"
- A state (genome) represented as a string over a finite alphabet
- Starts with k randomly generated states (population)
- An evaluation/fitness function for a state (phenome)
	- fn(genome) --> phenome
- Create next generation of states
	- randomly select parents with a probability proportional to their fitness
	- combine parents using a random crossover point
	- randomly mutate offspring with probability equal to mutation rate
	- Can sometimes find solutions local search cannot
- Shortcomings:
	- difficult to replicate between different problems
	- no convincing evidence that its better than hill climbing with random restarts



